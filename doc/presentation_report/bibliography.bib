
@article{duan_panoramic_2020,
	title = {Panoramic Image Generation: From 2-D Sketch to Spherical Image},
	volume = {14},
	issn = {1941-0484},
	url = {https://ieeexplore.ieee.org/abstract/document/8966243},
	doi = {10.1109/JSTSP.2020.2968772},
	shorttitle = {Panoramic Image Generation},
	abstract = {The 360-degree video/image, also called an omnidirectional video/image or panoramic video/image, is very important in some emerging areas such as virtual reality ({VR}). Therefore, corresponding image generation algorithms are urgently needed. However, existing image generation models mainly focus on 2-D images and do not consider the spherical structures of panoramic images. In this article, we propose a panoramic image generation method based on spherical convolution and generative adversarial networks, called spherical generative adversarial networks ({SGANs}). We adopt the sketch map as the input, which is a concise geometric structure representation of the panoramic image, e.g., comprising approximately 7\% of the pixels for a 583 × 1163 image. Through adversarial learning, a realistic-looking, plausible and high-fidelity spherical image can be obtained from the sparse sketch map. In particular, we build a dataset of the sketch maps using a visual computation-based sketching model. Then, by optimizing {SGANs} with {GAN} loss, feature matching loss and perceptual loss, realistic textures and details are recovered gradually. On one hand, it is an improvement using the sparse sketch map as input rather than the denser input, e.g., the features of the textures and colors. On the other hand, spherical convolution helps to remedy space-varying distortions of the planar projection. We conduct extensive experiments on some public panoramic image datasets and compare them with state-of-the-art techniques to validate the superior performance of the proposed approach.},
	pages = {194--208},
	number = {1},
	journaltitle = {{IEEE} Journal of Selected Topics in Signal Processing},
	author = {Duan, Yiping and Han, Chaoyi and Tao, Xiaoming and Geng, Bingrui and Du, Yunfei and Lu, Jianhua},
	urldate = {2024-03-14},
	date = {2020-01},
	note = {Conference Name: {IEEE} Journal of Selected Topics in Signal Processing},
	keywords = {Compressed sensing, Distortion, generative adversarial networks, Image coding, Image reconstruction, Image synthesis, Matching pursuit algorithms, Panoramic image generation, sparse sketch map, spherical convolution, Transforms},
	file = {IEEE Xplore Abstract Record:/Users/serms/Documents/Zotero/storage/ZHRURGIA/8966243.html:text/html;IEEE Xplore Full Text PDF:/Users/serms/Documents/Zotero/storage/W3Q5GUMA/Duan et al. - 2020 - Panoramic Image Generation From 2-D Sketch to Sph.pdf:application/pdf},
}

@article{gledhill_panoramic_2003,
	title = {Panoramic imaging—a review},
	volume = {27},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849303000384},
	doi = {10.1016/S0097-8493(03)00038-4},
	abstract = {Panoramic imaging has important implications in robotics, computer vision and virtual reality. This paper reviews representative work in the design and development of 2D/3D panoramic image capturing systems, the advancement of auto-calibration, registration and corresponding techniques, stereo vision, 3D reconstruction and image-based rendering. The paper discusses the above work within four parts of the panoramic imaging process: capturing system, image processing for panoramic imaging, image stitching and 3D reconstruction, image-based rending and visualisation. The design and development of a panoramic system pays careful attention to the following issues: image capturing, image registration, camera calibration, feature extraction, image understanding and image stitching. The objective of this review paper is to summarise and compare some of the methods in the various stages and identify research topics and applications, which are at the forefront of this exciting and challenge field.},
	pages = {435--445},
	number = {3},
	journaltitle = {Computers \& Graphics},
	shortjournal = {Computers \& Graphics},
	author = {Gledhill, Duke and Tian, Gui Yun and Taylor, Dave and Clarke, David},
	urldate = {2024-03-22},
	date = {2003-06-01},
	keywords = {3D modelling, 3D panoramic imaging, Correspondence, Image-based rendering, Stereo vision, Virtual environment},
	file = {Gledhill et al. - 2003 - Panoramic imaging—a review.pdf:/Users/serms/Documents/Zotero/storage/GBUG2AWF/Gledhill et al. - 2003 - Panoramic imaging—a review.pdf:application/pdf},
}

@article{yun_automatic_2019,
	title = {Automatic reconstruction method for high-contrast panoramic image from dental cone-beam {CT} data},
	volume = {175},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260719303384},
	doi = {10.1016/j.cmpb.2019.04.024},
	abstract = {Background and objective
Panoramic images reconstructed from dental cone beam {CT} ({CBCT}) data have been effectively used in dental clinics for disease diagnosis. Panoramic images generally have low contrast because excessive non-interest tissues participate in the reconstruction, which may affect the diagnosis. In this study, we developed a fully automatic reconstruction method to improve the global and detail contrast of panoramic images.
Methods
The proposed method consists of dental arch thickness detection, image synthesis, and image enhancement. First, the dental arch thickness is detected from an axial maximum intensity projection ({MIP}) image generated from the axial slices containing the teeth to reduce non-interest tissues in panoramic image reconstruction. Then, a new synthesis algorithm is proposed at image synthesis stage to reduce the effect of non-interest tissues on image contrast. Finally, an image enhancement algorithm is applied to the synthesized image to improve the detail contrast of the final panoramic image.
Results
A total of 129 real clinical dental {CBCT} data sets were used to test the proposed method. The panoramic images generated by three methods were subjectively scored by three experienced dentists who were blinded to the generated method. The evaluation of image contrast included the maxillary, mandible, teeth, and particular region (root canal, crown reconstruction, implants, and metal brackets). The overall image contrast score revealed that the proposed method scored the highest of 11.03 ± 2.46, followed by the ray sum and x-ray methods with corresponding scores of 6.4 ± 1.65 and 5.35 ± 1.56. The results of expert subjective scoring indicated that the image contrast of the panoramic image generated by the proposed method is higher than those of existing methods.
Conclusions
The proposed method provides a quick, effective and robust solution to improve the global and detail contrast of the panoramic image generated from dental {CBCT} data.},
	pages = {205--214},
	journaltitle = {Computer Methods and Programs in Biomedicine},
	shortjournal = {Computer Methods and Programs in Biomedicine},
	author = {Yun, Zhaoqiang and Yang, Shuo and Huang, Erliang and Zhao, Lei and Yang, Wei and Feng, Qianjin},
	urldate = {2024-03-22},
	date = {2019-07-01},
	keywords = {Cubic spline, Dental arch, Dental arch thickness, Image enhancement, Panoramic image},
	file = {Yun et al. - 2019 - Automatic reconstruction method for high-contrast .pdf:/Users/serms/Documents/Zotero/storage/2HBFQHWF/Yun et al. - 2019 - Automatic reconstruction method for high-contrast .pdf:application/pdf},
}

@online{noauthor_opencv_nodate,
	title = {{OpenCV}: Basic concepts of the homography explained with code},
	url = {https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html},
	urldate = {2024-04-01},
	file = {OpenCV\: Basic concepts of the homography explained with code:/Users/serms/Documents/Zotero/storage/WJAN6C3E/tutorial_homography.html:text/html},
}

@article{yan_heask_2014,
	title = {{HEASK}: Robust homography estimation based on appearance similarity and keypoint correspondences},
	volume = {47},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320313002112},
	doi = {10.1016/j.patcog.2013.05.007},
	shorttitle = {{HEASK}},
	abstract = {Accurate homography estimation is a classical problem with high industrial value and has been investigated extensively. Most previous homography estimation methods used either appearance similarity or keypoint correspondences to ﬁnd their best estimation. In this paper, a novel algorithm is proposed which integrates the advantages of the pixel-based and the feature-based homography estimation approaches. We elegantly combined the probability models of appearance similarity and keypoint correspondences in a Maximum Likelihood framework, which is named as Homography Estimation based on Appearance Similarity and Keypoint correspondences ({HEASK}). In the model of keypoint correspondences, the distribution of inlier location error is represented by a Laplacian distribution, which outperforms the previous Gaussian distribution in characterizing heavy-tailed distributions. And in the model of appearance similarity, the enhanced correlation coefﬁcient ({ECC}) is adopted for describing image similarity, and the distribution of {ECC} is studied and parametrically formulated using a truncated exponential distribution. The proposed model is solved based on an improved framework of random sample consensus ({RANSAC}). Several simulations summarize the performance of the proposed approach in objective quality measurement, subjective visual quality, and computation time. The experimental results demonstrate that the proposed approach can achieve more accurate homography estimation under different image transformation degrees and with different ratios of inlier keypoint correspondences as compared to the state-of-the-art works.},
	pages = {368--387},
	number = {1},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Yan, Qing and Xu, Yi and Yang, Xiaokang and Nguyen, Truong},
	urldate = {2024-04-01},
	date = {2014-01},
	langid = {english},
	file = {Yan et al. - 2014 - HEASK Robust homography estimation based on appea.pdf:/Users/serms/Documents/Zotero/storage/FX9VTF63/Yan et al. - 2014 - HEASK Robust homography estimation based on appea.pdf:application/pdf},
}

@inproceedings{mahajan_character_2019,
	location = {Sydney, Australia},
	title = {Character Keypoint-Based Homography Estimation in Scanned Documents for Efficient Information Extraction},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72815-054-3},
	url = {https://ieeexplore.ieee.org/document/8892840/},
	doi = {10.1109/ICDARW.2019.30060},
	eventtitle = {2019 International Conference on Document Analysis and Recognition Workshops ({ICDARW})},
	pages = {25--30},
	booktitle = {2019 International Conference on Document Analysis and Recognition Workshops ({ICDARW})},
	publisher = {{IEEE}},
	author = {Mahajan, Kushagra and Sharma, Monika and Vig, Lovekesh},
	urldate = {2024-04-01},
	date = {2019-09},
	file = {Mahajan et al. - 2019 - Character Keypoint-Based Homography Estimation in .pdf:/Users/serms/Documents/Zotero/storage/BX4NNHIL/Mahajan et al. - 2019 - Character Keypoint-Based Homography Estimation in .pdf:application/pdf},
}

@article{konrad_fisheyesuperpoint_2021,
	title = {{FisheyeSuperPoint}: Keypoint Detection and Description Network for Fisheye Images},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2103.00191},
	doi = {10.48550/ARXIV.2103.00191},
	shorttitle = {{FisheyeSuperPoint}},
	abstract = {Keypoint detection and description is a commonly used building block in computer vision systems particularly for robotics and autonomous driving. However, the majority of techniques to date have focused on standard cameras with little consideration given to fisheye cameras which are commonly used in urban driving and automated parking. In this paper, we propose a novel training and evaluation pipeline for fisheye images. We make use of {SuperPoint} as our baseline which is a self-supervised keypoint detector and descriptor that has achieved state-of-the-art results on homography estimation. We introduce a fisheye adaptation pipeline to enable training on undistorted fisheye images. We evaluate the performance on the {HPatches} benchmark, and, by introducing a fisheye based evaluation method for detection repeatability and descriptor matching correctness, on the Oxford {RobotCar} dataset.},
	author = {Konrad, Anna and Eising, Ciarán and Sistu, Ganesh and {McDonald}, John and Villing, Rudi and Yogamani, Senthil},
	urldate = {2024-04-01},
	date = {2021},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Robotics (cs.{RO})},
	file = {Texto completo:/Users/serms/Documents/Zotero/storage/IF2WUHV5/Konrad et al. - 2021 - FisheyeSuperPoint Keypoint Detection and Descript.pdf:application/pdf},
}

@article{yong_panoramic_2019,
	title = {Panoramic Background Image Generation for {PTZ} Cameras},
	volume = {28},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/8625516/},
	doi = {10.1109/TIP.2019.2894940},
	pages = {3162--3176},
	number = {7},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {Yong, Hongwei and Huang, Jianqiang and Xiang, Wangmeng and Hua, Xiansheng and Zhang, Lei},
	urldate = {2024-04-01},
	date = {2019-07},
	file = {Yong et al. - 2019 - Panoramic Background Image Generation for PTZ Came.pdf:/Users/serms/Documents/Zotero/storage/IUDGRUX8/Yong et al. - 2019 - Panoramic Background Image Generation for PTZ Came.pdf:application/pdf},
}
